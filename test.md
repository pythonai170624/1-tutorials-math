# ירידת גרדיאנט לפונקציה עם שני משתנים

## הפונקציה הנבחרת
נבחן את הפונקציה הפשוטה:

$$f(x,y) = x^2 + y^2$$

זוהי פונקציה קלאסית להדגמת ירידת גרדיאנט. הגרף שלה הוא פרבולואיד - מעין "קערה" תלת-ממדית שהמינימום שלה נמצא בנקודה $(0,0)$ עם ערך $f(0,0) = 0$.

## חישוב הגרדיאנט

הגרדיאנט של פונקציה הוא וקטור שמכיל את כל הנגזרות החלקיות:

$$\nabla f(x,y) = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right)$$

נחשב את הנגזרות החלקיות של הפונקציה שלנו:

* $\frac{\partial f}{\partial x} = \frac{\partial}{\partial x}(x^2 + y^2) = 2x$
* $\frac{\partial f}{\partial y} = \frac{\partial}{\partial y}(x^2 + y^2) = 2y$

לכן, הגרדיאנט של הפונקציה הוא:

$$\nabla f(x,y) = (2x, 2y)$$

## אלגוריתם ירידת הגרדיאנט

אלגוריתם ירידת הגרדיאנט פועל באופן הבא:

1. נבחר נקודת התחלה $(x_0, y_0)$
2. נבחר קצב למידה (learning rate) $\alpha$ (ערך חיובי קטן)
3. נחשב את הגרדיאנט בנקודה הנוכחית $\nabla f(x_n, y_n)$
4. נעדכן את המיקום שלנו לפי הנוסחה:
   $$(x_{n+1}, y_{n+1}) = (x_n, y_n) - \alpha \cdot \nabla f(x_n, y_n)$$
5. נחזור על צעדים 3-4 עד להתכנסות

## דוגמה מספרית

נתחיל מהנקודה $(x_0, y_0) = (2, 3)$ עם קצב למידה $\alpha = 0.1$.

### איטרציה 1:
* ערך הפונקציה בנקודת ההתחלה: $f(2,3) = 2^2 + 3^2 = 4 + 9 = 13$
* הגרדיאנט בנקודה זו: $\nabla f(2,3) = (2 \cdot 2, 2 \cdot 3) = (4, 6)$
* העדכון:
  $$(x_1, y_1) = (2, 3) - 0.1 \cdot (4, 6) = (2, 3) - (0.4, 0.6) = (1.6, 2.4)$$
* ערך הפונקציה בנקודה החדשה: $f(1.6, 2.4) = 1.6^2 + 2.4^2 = 2.56 + 5.76 = 8.32$
* שיפור: $13 - 8.32 = 4.68$

### איטרציה 2:
* הגרדיאנט בנקודה הנוכחית: $\nabla f(1.6, 2.4) = (2 \cdot 1.6, 2 \cdot 2.4) = (3.2, 4.8)$
* העדכון:
  $$(x_2, y_2) = (1.6, 2.4) - 0.1 \cdot (3.2, 4.8) = (1.6, 2.4) - (0.32, 0.48) = (1.28, 1.92)$$
* ערך הפונקציה: $f(1.28, 1.92) = 1.28^2 + 1.92^2 = 1.6384 + 3.6864 = 5.3248$
* שיפור: $8.32 - 5.3248 = 2.9952$

### איטרציה 3:
* הגרדיאנט: $\nabla f(1.28, 1.92) = (2.56, 3.84)$
* העדכון:
  $$(x_3, y_3) = (1.28, 1.92) - 0.1 \cdot (2.56, 3.84) = (1.024, 1.536)$$
* ערך הפונקציה: $f(1.024, 1.536) \approx 3.41$

### איטרציה 4:
* הגרדיאנט: $\nabla f(1.024, 1.536) = (2.048, 3.072)$
* העדכון:
  $$(x_4, y_4) = (1.024, 1.536) - 0.1 \cdot (2.048, 3.072) = (0.8192, 1.2288)$$
* ערך הפונקציה: $f(0.8192, 1.2288) \approx 2.18$

### המשך האיטרציות:
אם נמשיך את התהליך, נקבל את הנקודות הבאות:
* $(x_5, y_5) = (0.6554, 0.9830)$ עם $f \approx 1.40$
* $(x_6, y_6) = (0.5243, 0.7864)$ עם $f \approx 0.89$
* $(x_7, y_7) = (0.4194, 0.6291)$ עם $f \approx 0.57$
* ...

בסופו של דבר, הנקודה תתקרב יותר ויותר ל-$(0,0)$, שהיא נקודת המינימום של הפונקציה.

## ויזואליזציה של התהליך

אם נצייר את התהליך, נראה מסלול ספירלי שמתקרב למרכז הקערה:

```
      y
      ^
      |
  3.0 +   *
      |    \
  2.4 +     *
      |      \
  1.9 +       *
      |        \
  1.5 +         *
      |          \
  1.2 +           *
      |            \
  0.9 +             *
      |              \
  0.6 +               *
      |                \
  0.3 +                 \
      |                  \
  0.0 +-------------------*------------------> x
      0.0    1.0    2.0    3.0
```

## מאפיינים של הפונקציה $f(x,y) = x^2 + y^2$

הפונקציה $f(x,y) = x^2 + y^2$ היא פונקציה קמורה עם מינימום גלובלי יחיד. לכן, ירידת גרדיאנט תמיד תתכנס למינימום הגלובלי $(0,0)$ ללא קשר לנקודת ההתחלה, כל עוד קצב הלמידה מתאים.

עבור פונקציות מורכבות יותר, האלגוריתם עלול להתכנס למינימום מקומי במקום למינימום גלובלי, ולכן יש להשתמש בטכניקות מתקדמות יותר כמו Momentum, AdaGrad, או Adam.

## סיכום ירידת גרדיאנט

1. חשב את הגרדיאנט של הפונקציה
2. התקדם בצעדים קטנים בכיוון ההפוך לגרדיאנט
3. גודל הצעד נקבע על ידי קצב הלמידה $\alpha$
4. חזור על התהליך עד להתכנסות

ירידת גרדיאנט היא אלגוריתם פשוט אך רב-עוצמה שמשמש ככלי יסודי באופטימיזציה ובלמידת מכונה.
