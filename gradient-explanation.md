# גרדיאנט (Gradient)

גרדיאנט הוא מושג מתמטי המייצג את כיוון העלייה המרבית של פונקציה. במילים פשוטות, הגרדיאנט הוא וקטור המצביע לכיוון שבו הפונקציה גדלה בקצב המהיר ביותר.

## תכונות מתמטיות

- הגרדיאנט של פונקציה $f$ מסומן כ-$\nabla f$ (נקרא "נבלה של $f$")
- עבור פונקציה $f(x_1, x_2, ..., x_n)$ של מספר משתנים, הגרדיאנט הוא וקטור המכיל את כל הנגזרות החלקיות:

$$\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n} \right)$$

- גודלו של הגרדיאנט מייצג את קצב השינוי של הפונקציה
- כיוונו של הגרדיאנט מצביע לכיוון העלייה התלולה ביותר של הפונקציה
- הגרדיאנט תמיד מאונך למשטח קווי הגובה של הפונקציה

## דוגמה

עבור הפונקציה $f(x, y) = x^2 + y^2$, הגרדיאנט יהיה:

$$\nabla f = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right) = (2x, 2y)$$

בנקודה $(3, 4)$, הגרדיאנט יהיה $\nabla f = (6, 8)$, כלומר וקטור המצביע בכיוון העלייה המהירה ביותר מנקודה זו.

# ירידת גרדיאנט (Gradient Descent)

ירידת גרדיאנט היא אלגוריתם אופטימיזציה פופולרי המשמש למציאת המינימום של פונקציה. האלגוריתם עובד על ידי התקדמות איטרטיבית בכיוון ההפוך לגרדיאנט של הפונקציה.

## הרעיון המרכזי

מכיוון שהגרדיאנט מצביע לכיוון העלייה המרבית של הפונקציה, התנועה בכיוון ההפוך לגרדיאנט תוביל לירידה המהירה ביותר בערך הפונקציה. זהו הרעיון המרכזי מאחורי אלגוריתם ירידת הגרדיאנט.

## האלגוריתם

1. התחל בנקודה התחלתית כלשהי $\theta_0$
2. חשב את הגרדיאנט של הפונקציה בנקודה הנוכחית $\nabla f(\theta_t)$
3. עדכן את הפרמטרים על ידי תנועה בכיוון ההפוך לגרדיאנט:
   $\theta_{t+1} = \theta_t - \alpha \nabla f(\theta_t)$
   כאשר $\alpha$ היא קצב הלמידה (learning rate)
4. חזור על צעדים 2-3 עד להתכנסות

## קצב הלמידה (Learning Rate)

קצב הלמידה $\alpha$ הוא היפר-פרמטר חשוב באלגוריתם:
- אם $\alpha$ גדול מדי, האלגוריתם עלול לדלג על המינימום ולא להתכנס
- אם $\alpha$ קטן מדי, ההתכנסות תהיה איטית מאוד
- בחירת קצב למידה אופטימלי היא אתגר משמעותי בהפעלת האלגוריתם

## וריאציות של ירידת גרדיאנט

קיימות מספר גרסאות מתקדמות של אלגוריתם ירידת הגרדיאנט:

1. **ירידת גרדיאנט סטוכסטית (SGD)** - משתמשת בדגימה אקראית של נתונים בכל איטרציה
2. **ירידת גרדיאנט עם תנע (Momentum)** - מוסיפה "תנע" לתנועה כדי להתגבר על מינימום מקומי
3. **AdaGrad** - מתאימה את קצב הלמידה לכל פרמטר בנפרד
4. **Adam** - משלבת את היתרונות של תנע והתאמת קצב למידה

## שימושים בלמידת מכונה

ירידת גרדיאנט היא אבן היסוד של רוב אלגוריתמי הלמידה במדעי הנתונים ולמידת מכונה:
- אימון רשתות נוירונים
- למידת מודלים ליניאריים
- מכונות וקטורי תמיכה (SVM)
- ועוד רבים אחרים

האלגוריתם מאפשר מציאת פרמטרים אופטימליים למודל על ידי מזעור פונקציית שגיאה או הפסד.
